For this task, we based the code on the provided paper "Bayesian Optimization with Unknown Constraints" from Gelbart et. al. as well as the lecture notes and examples. For the implementation of the "acquisition_function" we tested different approaches, namely GP-UCB and "Expected improvement" (EI) in addition to our implementation of their "constraint-weighted" versions as descriped in the paper. Our best version was based on such a constraint-weighted version: We computed the cdf of a normal for the value 0 under the predicted constraint-mu and -sigma for each iteration and deducted a penalty term from the acquisition function value (hyperparam: 3) if the probability of "not fulfilling the constraint" was high before multipliing the calculated probability with the standard EI. For each new datapoint the gaussian process regression was updated (new .fit(X,y) call) and based on this new fit, the prediction for the next point was computed. The solution-point was found iterating through all previous points and only returning the value if the constraint was "safely" fullfilled (based on provided c(x) that was earlier appended to the self.previous_points). This meant that we set the threshold as constraint < -0.1*(var_target + var_constraint) instead of simply <0. This way, we could reduce the risk of getting a regret of 1.