{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import typing\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from tqdm import trange\n",
    "\n",
    "import abc\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f707c02b3b0>"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor([1.0]).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParameterDistribution(torch.nn.Module, metaclass=abc.ABCMeta):\n",
    "    \"\"\"\n",
    "    Abstract class that models a distribution over model parameters,\n",
    "    usable for Bayes by backprop.\n",
    "    You can implement this class using any distribution you want\n",
    "    and try out different priors and variational posteriors.\n",
    "    All torch.nn.Parameter that you add in the __init__ method of this class\n",
    "    will automatically be registered and know to PyTorch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def log_likelihood(self, values: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Calculate the log-likelihood of the given values\n",
    "        :param values: Values to calculate the log-likelihood on\n",
    "        :return: Log-likelihood\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def sample(self) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Sample from this distribution.\n",
    "        Note that you only need to implement this method for variational posteriors, not priors.\n",
    "\n",
    "        :return: Sample from this distribution. The sample shape depends on your semantics.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def forward(self, values: torch.Tensor) -> torch.Tensor:\n",
    "        # DO NOT USE THIS METHOD\n",
    "        # We only implement it since torch.nn.Module requires a forward method\n",
    "        warnings.warn('ParameterDistribution should not be called! Use its explicit methods!')\n",
    "        return self.log_likelihood(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnivariateGaussian(ParameterDistribution):\n",
    "    \"\"\"\n",
    "    Univariate Gaussian distribution.\n",
    "    For multivariate data, this assumes all elements to be i.i.d.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mu: torch.Tensor, sigma: torch.Tensor):\n",
    "        super(\n",
    "            UnivariateGaussian, self\n",
    "        ).__init__()  # always make sure to include the super-class init call!\n",
    "        assert mu.size() == () and sigma.size() == ()\n",
    "        assert sigma > 0\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def log_likelihood(self, values: torch.Tensor) -> torch.Tensor:\n",
    "        ll = torch.sum(torch.log(((1. / np.sqrt(2.0 * np.pi))/self.sigma) * torch.exp(- (values - self.mu) ** 2 / (2.0 * self.sigma ** 2))))\n",
    "        return ll  # clip to avoid numerical issues\n",
    "\n",
    "    def sample(self) -> torch.Tensor:\n",
    "        return torch.distributions.Normal(self.mu, self.sigma).sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_norm = UnivariateGaussian(torch.tensor(0.0), torch.tensor(1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_values = torch.Tensor([1.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-18.4838, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_norm.log_likelihood(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-18.4838, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_norm.log_likelihood(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-18.4838, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(torch.distributions.Normal(0,1).log_prob(weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-18.4838, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_norm.log_likelihood(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_features = 4\n",
    "in_features = 5\n",
    "weight_rho = nn.Parameter(torch.Tensor(out_features, in_features).uniform_(-3., -3.))\n",
    "weight_mu = nn.Parameter(torch.Tensor(out_features, in_features).normal_(0., .1))\n",
    "bias_mu = nn.Parameter(torch.Tensor(out_features).normal_(0., .1))\n",
    "bias_rho = nn.Parameter(torch.Tensor(out_features).uniform_(-3., -3.))\n",
    "\n",
    "weight_sigma = torch.log(1. + torch.exp(weight_rho))\n",
    "bias_sigma = torch.log(1. + torch.exp(bias_rho))\n",
    "epsilon_weight = torch.autograd.Variable(torch.Tensor(out_features, in_features).normal_(0., 1.))\n",
    "epsilon_bias = torch.autograd.Variable(torch.Tensor(out_features).normal_(0., 1.))\n",
    "weight = weight_mu + weight_sigma * epsilon_weight\n",
    "bias = bias_mu + bias_sigma * epsilon_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-18.4838, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.log(gaussian(weight, 0, 1.0)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2531, grad_fn=<LogBackward>)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GAUSSIAN_SCALER = 1. / np.sqrt(2.0 * np.pi)\n",
    "def gaussian(x, mu, sigma):\n",
    "    bell = torch.exp(- (x - mu) ** 2 / (2.0 * sigma ** 2))\n",
    "    return torch.clamp(GAUSSIAN_SCALER / sigma * bell, 1e-10, 1.)  # clip to avoid numerical issues\n",
    "\n",
    "torch.log(gaussian(weight, 0, 1.0).sum() + gaussian(bias, 0, 1.0).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6915,  0.4607,  0.0377, -1.4025],\n",
       "        [-0.2379,  0.2781, -1.3480, -0.7980],\n",
       "        [ 0.0922,  1.1334, -1.2153, -2.1855],\n",
       "        [ 0.5999,  0.4441,  0.4513,  0.8644],\n",
       "        [-0.1280, -2.4436, -0.8932, -0.3862],\n",
       "        [-0.7883,  0.0139, -0.1614,  0.3438],\n",
       "        [ 0.8921, -0.1609,  1.4146, -1.7340],\n",
       "        [ 0.0317,  0.5293, -0.6766,  1.0246],\n",
       "        [-1.3761, -0.3815, -0.3808,  0.6661],\n",
       "        [-0.8763, -0.9672,  0.6168, -1.2584]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.distributions.Normal(0,1).sample((10,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_mu = torch.nn.Parameter(torch.zeros(out_features, in_features).uniform_(-0.0005, 0.0005))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultivariateDiagonalGaussian(ParameterDistribution):\n",
    "    \"\"\"\n",
    "    Multivariate diagonal Gaussian distribution,\n",
    "    i.e., assumes all elements to be independent Gaussians\n",
    "    but with different means and standard deviations.\n",
    "    This parameterizes the standard deviation via a parameter rho as\n",
    "    sigma = softplus(rho).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mu: torch.Tensor, rho: torch.Tensor):\n",
    "        super(\n",
    "            MultivariateDiagonalGaussian, self\n",
    "        ).__init__()  # always make sure to include the super-class init call!\n",
    "        assert mu.size() == rho.size()\n",
    "        self.mu = mu\n",
    "        self.rho = rho\n",
    "\n",
    "    def log_likelihood(self, values: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.sum(torch.distributions.Normal(self.mu, self.rho).log_prob(values))\n",
    "\n",
    "    def sample(self) -> torch.Tensor:\n",
    "        return torch.distributions.Normal(self.mu, self.sigma).sample(mu.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Bayes Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianLayerOwn(nn.Module):\n",
    "    \"\"\"\n",
    "    Module implementing a single Bayesian feedforward layer.\n",
    "    It maintains a prior and variational posterior for the weights (and biases)\n",
    "    and uses sampling to approximate the gradients via Bayes by backprop.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features: int, out_features: int, bias: bool = True):\n",
    "        \"\"\"\n",
    "        Create a BayesianLayer.\n",
    "\n",
    "        :param in_features: Number of input features\n",
    "        :param out_features: Number of output features\n",
    "        :param bias: If true, use a bias term (i.e., affine instead of linear transformation)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.use_bias = bias\n",
    "\n",
    "        # TODO: Create a suitable prior for weights and biases as an instance of ParameterDistribution.\n",
    "        #  You can use the same prior for both weights and biases, but are free to experiment with different priors.\n",
    "        #  You can create constants using torch.tensor(...).\n",
    "        #  Do NOT use torch.Parameter(...) here since the prior should not be optimized!\n",
    "        #  Example: self.prior = MyPrior(torch.tensor(0.0), torch.tensor(1.0))\n",
    "\n",
    "        self.prior_mu = 0\n",
    "        self.prior_sigma = 1.0\n",
    "\n",
    "        self.prior = UnivariateGaussian(\n",
    "            torch.tensor(self.prior_mu), torch.tensor(self.prior_sigma)\n",
    "        )\n",
    "        assert isinstance(self.prior, ParameterDistribution)\n",
    "        assert not any(\n",
    "            True for _ in self.prior.parameters()\n",
    "        ), \"Prior cannot have parameters\"\n",
    "\n",
    "        # TODO: Create a suitable variational posterior for weights as an instance of ParameterDistribution.\n",
    "        #  You need to create separate ParameterDistribution instances for weights and biases,\n",
    "        #  but can use the same family of distributions if you want.\n",
    "        #  IMPORTANT: You need to create a nn.Parameter(...) for each parameter\n",
    "        #  and add those parameters as an attribute in the ParameterDistribution instances.\n",
    "        #  If you forget to do so, PyTorch will not be able to optimize your variational posterior.\n",
    "        #  Example: self.weights_var_posterior = MyPosterior(\n",
    "        #      torch.nn.Parameter(torch.zeros((out_features, in_features))),\n",
    "        #      torch.nn.Parameter(torch.ones((out_features, in_features)))\n",
    "        #  )\n",
    "        self.weight_mu = torch.nn.Parameter(\n",
    "            torch.zeros(out_features, in_features).uniform_(-0.0005, 0.0005)\n",
    "        )\n",
    "        self.weight_logsigma = torch.nn.Parameter(\n",
    "            torch.zeros(out_features, in_features).uniform_(-2.56, -2.55)\n",
    "        )\n",
    "        self.weights_var_posterior = MultivariateDiagonalGaussian(\n",
    "            self.weight_mu, self.weight_logsigma\n",
    "        )\n",
    "\n",
    "        assert isinstance(self.weights_var_posterior, ParameterDistribution)\n",
    "        assert any(\n",
    "            True for _ in self.weights_var_posterior.parameters()\n",
    "        ), \"Weight posterior must have parameters\"\n",
    "\n",
    "        if self.use_bias:\n",
    "            # TODO: As for the weights, create the bias variational posterior instance here.\n",
    "            #  Make sure to follow the same rules as for the weight variational posterior.\n",
    "            self.bias_mu = torch.nn.Parameter(\n",
    "                torch.zeros(out_features).normal_(-0.0005, 0.0005)\n",
    "            )\n",
    "            self.bias_logsigma = torch.nn.Parameter(\n",
    "                torch.zeros(out_features).uniform_(-2.56, -2.55)\n",
    "            )\n",
    "            self.bias_var_posterior = MultivariateDiagonalGaussian(\n",
    "                self.bias_mu, self.bias_logsigma\n",
    "            )\n",
    "            assert isinstance(self.bias_var_posterior, ParameterDistribution)\n",
    "            assert any(\n",
    "                True for _ in self.bias_var_posterior.parameters()\n",
    "            ), \"Bias posterior must have parameters\"\n",
    "        else:\n",
    "            self.bias_var_posterior = None\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Perform one forward pass through this layer.\n",
    "        If you need to sample weights from the variational posterior, you can do it here during the forward pass.\n",
    "        Just make sure that you use the same weights to approximate all quantities\n",
    "        present in a single Bayes by backprop sampling step.\n",
    "\n",
    "        :param inputs: Flattened input images as a (batch_size, in_features) float tensor\n",
    "        :return: 3-tuple containing\n",
    "            i) transformed features using stochastic weights from the variational posterior,\n",
    "            ii) sample of the log-prior probability, and\n",
    "            iii) sample of the log-variational-posterior probability\n",
    "        \"\"\"\n",
    "        # TODO: Perform a forward pass as described in this method's docstring.\n",
    "        #  Make sure to check whether `self.use_bias` is True,\n",
    "        #  and if yes, include the bias as well.\n",
    "\n",
    "        # Sample weights and bias #\n",
    "        crt_sigma = torch.exp(self.weight_logsigma)\n",
    "        # Weight sampling\n",
    "        normal_dist = torch.distributions.Normal(0, 1)\n",
    "        ## Step 1 from paper\n",
    "        epsilon_weight = normal_dist.sample(self.weight_mu.shape)\n",
    "        ## Step 2 of paper\n",
    "        weights = self.weight_mu + torch.log(1.0 + crt_sigma * epsilon_weight)\n",
    "\n",
    "        # Bias sampling\n",
    "        if self.use_bias:\n",
    "            bias_sigma = torch.log(1.0 + torch.exp(self.bias_logsigma))\n",
    "            ## Step 1 from paper\n",
    "            epsilon_bias = normal_dist.sample(self.bias_mu.shape)\n",
    "            ## Step 2 of paper\n",
    "            bias = self.bias_mu + bias_sigma * epsilon_bias\n",
    "            print(bias)\n",
    "            print(weights)\n",
    "            # LOG PRIOR (WEIGHTS ONLY)\n",
    "            log_prior = self.prior.log_likelihood(weights) + self.prior.log_likelihood(\n",
    "                bias\n",
    "            )\n",
    "            log_variational_posterior = (\n",
    "                MultivariateDiagonalGaussian(self.weight_mu.data, crt_sigma).log_likelihood(\n",
    "                    weights\n",
    "                )\n",
    "                + torch.distributions.Normal(\n",
    "                    self.bias_mu.data, torch.log(1 + torch.exp(self.bias_logsigma))\n",
    "                )\n",
    "                .log_prob(bias)\n",
    "                .sum()\n",
    "            )\n",
    "        else:\n",
    "            bias = None\n",
    "            log_prior = self.prior.log_likelihood(weights)\n",
    "            log_variational_posterior = MultivariateDiagonalGaussian(\n",
    "                self.weight_mu.data, crt_sigma\n",
    "            ).log_likelihood(weights)\n",
    "\n",
    "        return F.linear(inputs, weights, bias), log_prior, log_variational_posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianLayerOld(torch.nn.Module):\n",
    "    '''\n",
    "    Module implementing a single Bayesian feedforward layer.\n",
    "    The module performs Bayes-by-backprop, that is, mean-field\n",
    "    variational inference. It keeps prior and posterior weights\n",
    "    (and biases) and uses the reparameterization trick for sampling.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, bias=True):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.use_bias = bias\n",
    "\n",
    "        # TODO: enter your code here\n",
    "        self.prior_mu = 0\n",
    "        self.prior_sigma = 0.1\n",
    "\n",
    "        self.prior = torch.distributions.Normal(self.prior_mu, self.prior_sigma)\n",
    "\n",
    "\n",
    "        self.weight_mu = nn.Parameter(torch.zeros(output_dim, input_dim).uniform_(-0.0005, 0.0005))\n",
    "        self.weight_logsigma = nn.Parameter(torch.zeros(output_dim, input_dim).uniform_(-2.56,-2.55))\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias_mu = nn.Parameter(torch.zeros(output_dim).uniform_(-0.0005,0.0005))\n",
    "            self.bias_logsigma =  nn.Parameter(torch.zeros(output_dim).uniform_(-2.56,-2.55))\n",
    "\n",
    "        else:\n",
    "            self.register_parameter('bias_mu', None)\n",
    "            self.register_parameter('bias_logsigma', None)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        self.num_batches = round(60000 / inputs.shape[0])\n",
    "\n",
    "        # SAMPLE WEIGHTS\n",
    "        w_epsilon = torch.distributions.Normal(0, 1).sample(self.weight_mu.shape)\n",
    "        self.w = self.weight_mu + torch.log(1 + torch.exp(self.weight_logsigma)) * w_epsilon\n",
    "        print(w_epsilon)\n",
    "\n",
    "\n",
    "        # LOG PRIOR (WEIGHTS ONLY)\n",
    "        w_log_prior = self.prior.log_prob(self.w)\n",
    "        self.log_prior = torch.sum(w_log_prior)\n",
    "\n",
    "\n",
    "        # LOG POSTERIOR (WEIGHTS ONLY)\n",
    "        self.w_post = torch.distributions.Normal(self.weight_mu.data, torch.log(1 + torch.exp(self.weight_logsigma)))\n",
    "        self.log_post = self.w_post.log_prob(self.w).sum()\n",
    "\n",
    "\n",
    "        # ADDING THE BIAS TERM\n",
    "        if self.use_bias:\n",
    "            # SAMPLE BIAS\n",
    "            b_epsilon = torch.distributions.Normal(0, 1).sample(self.bias_mu.shape)\n",
    "            self.b = self.bias_mu + torch.log(1 + torch.exp(self.bias_logsigma)) * b_epsilon\n",
    "            print(self.b)\n",
    "            print(self.w)\n",
    "            # LOG PRIOR AND POSTERIOR OF THE BIAS\n",
    "            b_log_prior = self.prior.log_prob(self.b)\n",
    "            self.log_prior += torch.sum(b_log_prior)\n",
    "\n",
    "            self.b_post = torch.distributions.Normal(self.bias_mu.data, torch.log(1 + torch.exp(self.bias_logsigma)))\n",
    "            self.log_post += self.b_post.log_prob(self.b).sum()\n",
    "\n",
    "\n",
    "        if self.use_bias:\n",
    "            return F.linear(inputs, self.w, self.b), self.log_prior, self.log_post\n",
    "        else:\n",
    "            bias = None\n",
    "            return torch.relu(torch.mm(inputs, self.w)), self.log_prior, self.log_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "vorlageBL = BayesianLayerOld(5, 5, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenerBL = BayesianLayerOwn(5, 5, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-2.5592, -2.5562, -2.5548, -2.5543, -2.5538], requires_grad=True)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vorlageBL.bias_logsigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-2.5514, -2.5536, -2.5526, -2.5532, -2.5562], requires_grad=True)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigenerBL.bias_logsigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = torch.Tensor([0.4,0.6,0.1,0.23,0.85])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0746, -0.0119, -0.0190, -0.0810,  0.0007], grad_fn=<AddBackward0>)\n",
      "tensor([[ 0.0121,  0.0084, -0.0614,  0.0476, -0.1427],\n",
      "        [-0.0420,  0.0400,  0.0307,  0.0745, -0.0929],\n",
      "        [ 0.0732, -0.0373, -0.0959,  0.0331, -0.0004],\n",
      "        [ 0.1010, -0.0209,  0.0210, -0.0024, -0.0794],\n",
      "        [-0.0090,  0.0150, -0.0546,  0.1267,  0.0260]], grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([-0.0320, -0.0635, -0.0144, -0.1191,  0.0519], grad_fn=<AddBackward0>),\n",
       " tensor(35.8993, grad_fn=<AddBackward0>),\n",
       " tensor(40.1668, grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vorlageBL.forward(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0422, -0.0047, -0.0463, -0.0283, -0.0970], grad_fn=<AddBackward0>)\n",
      "tensor([[ 0.0329, -0.0405, -0.0149, -0.0155,  0.0120],\n",
      "        [ 0.0651,  0.0181,  0.0183, -0.0197, -0.0201],\n",
      "        [ 0.0366,  0.0263, -0.1335, -0.0462,  0.0459],\n",
      "        [ 0.0339, -0.0073, -0.1104, -0.0018, -0.1168],\n",
      "        [ 0.0450, -0.0943,  0.0025,  0.0164, -0.0746]], grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.0362,  0.0123, -0.0009, -0.1299, -0.1949], grad_fn=<AddBackward0>),\n",
       " tensor(-27.6137, grad_fn=<AddBackward0>),\n",
       " tensor(41.6318, grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigenerBL.forward(input_tensor)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9b00745acea50bfbd0f952ed96a3af3e23f07a15e06f6e82e3ab66a13ea81a2e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('paiTask2': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
